{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['not_cyberbullying', 'gender', 'religion', 'other_cyberbullying',\n",
       "       'age', 'ethnicity'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/AsukaaNao/datasets/refs/heads/main/cyberbullying_tweets.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "data['cyberbullying_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Christian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after SMOTE: [31832 31832]\n",
      "Class Weights: {0: 3.001384518565135, 1: 0.5999446499106851}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christian\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1990/1990 - 136s - 68ms/step - accuracy: 0.7540 - loss: 0.5119 - val_accuracy: 0.6162 - val_loss: 0.9552\n",
      "Epoch 2/20\n",
      "1990/1990 - 132s - 66ms/step - accuracy: 0.8109 - loss: 0.3825 - val_accuracy: 0.6820 - val_loss: 0.7333\n",
      "Epoch 3/20\n",
      "1990/1990 - 132s - 66ms/step - accuracy: 0.8488 - loss: 0.3001 - val_accuracy: 0.6953 - val_loss: 0.7765\n",
      "Epoch 4/20\n",
      "1990/1990 - 142s - 71ms/step - accuracy: 0.8782 - loss: 0.2404 - val_accuracy: 0.7504 - val_loss: 0.6637\n",
      "Epoch 5/20\n",
      "1990/1990 - 128s - 64ms/step - accuracy: 0.8980 - loss: 0.2002 - val_accuracy: 0.7401 - val_loss: 0.8326\n",
      "Epoch 6/20\n",
      "1990/1990 - 137s - 69ms/step - accuracy: 0.9127 - loss: 0.1716 - val_accuracy: 0.7654 - val_loss: 0.7531\n",
      "Epoch 7/20\n",
      "1990/1990 - 130s - 65ms/step - accuracy: 0.9243 - loss: 0.1470 - val_accuracy: 0.7563 - val_loss: 0.8991\n",
      "Epoch 8/20\n",
      "1990/1990 - 132s - 66ms/step - accuracy: 0.9341 - loss: 0.1285 - val_accuracy: 0.7389 - val_loss: 0.9268\n",
      "Epoch 9/20\n",
      "1990/1990 - 126s - 63ms/step - accuracy: 0.9403 - loss: 0.1174 - val_accuracy: 0.7426 - val_loss: 1.3415\n",
      "Epoch 10/20\n",
      "1990/1990 - 130s - 65ms/step - accuracy: 0.9454 - loss: 0.1071 - val_accuracy: 0.7660 - val_loss: 1.0732\n",
      "Epoch 11/20\n",
      "1990/1990 - 134s - 67ms/step - accuracy: 0.9494 - loss: 0.0984 - val_accuracy: 0.7253 - val_loss: 1.4252\n",
      "Epoch 12/20\n",
      "1990/1990 - 132s - 66ms/step - accuracy: 0.9535 - loss: 0.0919 - val_accuracy: 0.7477 - val_loss: 1.4259\n",
      "Epoch 13/20\n",
      "1990/1990 - 135s - 68ms/step - accuracy: 0.9567 - loss: 0.0856 - val_accuracy: 0.7440 - val_loss: 1.3321\n",
      "Epoch 14/20\n",
      "1990/1990 - 134s - 67ms/step - accuracy: 0.9603 - loss: 0.0789 - val_accuracy: 0.7705 - val_loss: 1.3718\n",
      "Epoch 15/20\n",
      "1990/1990 - 131s - 66ms/step - accuracy: 0.9605 - loss: 0.0805 - val_accuracy: 0.7611 - val_loss: 1.2917\n",
      "Epoch 16/20\n",
      "1990/1990 - 130s - 65ms/step - accuracy: 0.9635 - loss: 0.0728 - val_accuracy: 0.7608 - val_loss: 1.4366\n",
      "Epoch 17/20\n",
      "1990/1990 - 131s - 66ms/step - accuracy: 0.9643 - loss: 0.0697 - val_accuracy: 0.7225 - val_loss: 1.8887\n",
      "Epoch 18/20\n",
      "1990/1990 - 131s - 66ms/step - accuracy: 0.9645 - loss: 0.0696 - val_accuracy: 0.7657 - val_loss: 1.5532\n",
      "Epoch 19/20\n",
      "1990/1990 - 129s - 65ms/step - accuracy: 0.9669 - loss: 0.0654 - val_accuracy: 0.7611 - val_loss: 1.6272\n",
      "Epoch 20/20\n",
      "1990/1990 - 131s - 66ms/step - accuracy: 0.9667 - loss: 0.0673 - val_accuracy: 0.7574 - val_loss: 1.7058\n",
      "299/299 - 5s - 16ms/step - accuracy: 0.7574 - loss: 1.7058\n",
      "Test Loss: 1.7057838439941406\n",
      "Test Accuracy: 0.7574169039726257\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step\n",
      "Prediction for sample text: Not Cyberbullying\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/AsukaaNao/datasets/refs/heads/main/cyberbullying_tweets.csv\"\n",
    "data = pd.read_csv(url)\n",
    "data['tweet_text'] = data['tweet_text'].astype(str)\n",
    "\n",
    "# Text preprocessing\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)  # Remove non-alphabetical characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.split()  # Tokenize\n",
    "    text = [word for word in text if word not in stop_words]  # Remove stopwords\n",
    "    return \" \".join(text)\n",
    "\n",
    "data['tweet_text'] = data['tweet_text'].apply(preprocess_text)\n",
    "\n",
    "# Extract content and label\n",
    "contents = data['tweet_text']\n",
    "labels = data['cyberbullying_type']\n",
    "\n",
    "# Encode labels (Binary Classification: Cyberbullying or Not)\n",
    "labels = labels.apply(lambda x: 0 if x == \"not_cyberbullying\" else 1)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(contents, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Padding sequences\n",
    "max_length = 100\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "# Oversample minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_padded, y_train)\n",
    "\n",
    "# Check new class distribution\n",
    "print(f\"Class distribution after SMOTE: {np.bincount(y_train_resampled)}\")\n",
    "\n",
    "# Class weights for imbalanced dataset\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(labels),\n",
    "    y=labels\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(f\"Class Weights: {class_weights}\")\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=max_length),\n",
    "    Bidirectional(LSTM(128, return_sequences=False)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Early stopping (di uncomment kalo mau ngestop halfway)\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_resampled, y_train_resampled,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_padded, y_test),\n",
    "    class_weight=class_weights,\n",
    "    # callbacks=[early_stopping],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test_padded, y_test, verbose=2)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Prediction function\n",
    "def predict_cyberbullying(content):\n",
    "    content = preprocess_text(content)\n",
    "    seq = tokenizer.texts_to_sequences([content])\n",
    "    padded = pad_sequences(seq, maxlen=max_length, padding='post')\n",
    "    prediction = model.predict(padded)\n",
    "    return \"Cyberbullying\" if prediction[0][0] > 0.5 else \"Not Cyberbullying\"\n",
    "\n",
    "# Test the prediction function\n",
    "sample_text = \"You are so dumb and useless!\"\n",
    "print(f\"Prediction for sample text: {predict_cyberbullying(sample_text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Prediction for sample text: Not Trolling\n"
     ]
    }
   ],
   "source": [
    "# Prediction function\n",
    "def predict_trolling(content):\n",
    "    content = preprocess_text(content)\n",
    "    seq = tokenizer.texts_to_sequences([content])\n",
    "    padded = pad_sequences(seq, maxlen=max_length, padding='post')\n",
    "    prediction = model.predict(padded)\n",
    "    return \"Trolling\" if prediction[0][0] > 0.5 else \"Not Trolling\"\n",
    "\n",
    "# Test the prediction function\n",
    "sample_text = \"You are the worst! Nobody likes you.\"\n",
    "print(f\"Prediction for sample text: {predict_trolling(sample_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data as x_test (blm ganti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Christian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 0.8211265292716972, 1: 1.2785093326514958}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christian\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "626/626 - 47s - 75ms/step - accuracy: 0.7130 - loss: 0.5268\n",
      "Epoch 2/20\n",
      "626/626 - 45s - 72ms/step - accuracy: 0.8639 - loss: 0.3061\n",
      "Epoch 3/20\n",
      "626/626 - 42s - 68ms/step - accuracy: 0.9239 - loss: 0.1827\n",
      "Epoch 4/20\n",
      "626/626 - 40s - 64ms/step - accuracy: 0.9531 - loss: 0.1212\n",
      "Epoch 5/20\n",
      "626/626 - 44s - 71ms/step - accuracy: 0.9638 - loss: 0.0934\n",
      "Epoch 6/20\n",
      "626/626 - 43s - 69ms/step - accuracy: 0.9686 - loss: 0.0779\n",
      "Epoch 7/20\n",
      "626/626 - 41s - 65ms/step - accuracy: 0.9753 - loss: 0.0645\n",
      "Epoch 8/20\n",
      "626/626 - 40s - 64ms/step - accuracy: 0.9756 - loss: 0.0597\n",
      "Epoch 9/20\n",
      "626/626 - 40s - 64ms/step - accuracy: 0.9778 - loss: 0.0564\n",
      "Epoch 10/20\n",
      "626/626 - 45s - 71ms/step - accuracy: 0.9804 - loss: 0.0477\n",
      "Epoch 11/20\n",
      "626/626 - 45s - 72ms/step - accuracy: 0.9796 - loss: 0.0495\n",
      "Epoch 12/20\n",
      "626/626 - 41s - 65ms/step - accuracy: 0.9815 - loss: 0.0425\n",
      "Epoch 13/20\n",
      "626/626 - 40s - 64ms/step - accuracy: 0.9840 - loss: 0.0396\n",
      "Epoch 14/20\n",
      "626/626 - 44s - 70ms/step - accuracy: 0.9843 - loss: 0.0371\n",
      "Epoch 15/20\n",
      "626/626 - 37s - 59ms/step - accuracy: 0.9826 - loss: 0.0406\n",
      "Epoch 16/20\n",
      "626/626 - 35s - 57ms/step - accuracy: 0.9854 - loss: 0.0342\n",
      "Epoch 17/20\n",
      "626/626 - 35s - 56ms/step - accuracy: 0.9851 - loss: 0.0357\n",
      "Epoch 18/20\n",
      "626/626 - 35s - 56ms/step - accuracy: 0.9843 - loss: 0.0359\n",
      "Epoch 19/20\n",
      "626/626 - 38s - 61ms/step - accuracy: 0.9853 - loss: 0.0339\n",
      "Epoch 20/20\n",
      "626/626 - 43s - 69ms/step - accuracy: 0.9864 - loss: 0.0294\n",
      "626/626 - 10s - 16ms/step - accuracy: 0.9887 - loss: 0.0252\n",
      "Test Loss: 0.025235412642359734\n",
      "Test Accuracy: 0.9887005686759949\n",
      "\u001b[1m626/626\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step\n",
      "Predictions have been saved to 'trolling_predictions.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/AsukaaNao/datasets/refs/heads/main/trolling_data.csv\"\n",
    "data = pd.read_csv(url)\n",
    "data['content'] = data['content'].astype(str)\n",
    "\n",
    "# Text preprocessing\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)  # Remove non-alphabetical characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.split()  # Tokenize\n",
    "    text = [word for word in text if word not in stop_words]  # Remove stopwords\n",
    "    return \" \".join(text)\n",
    "\n",
    "data['content'] = data['content'].apply(preprocess_text)\n",
    "\n",
    "# Extract content and label\n",
    "contents = data['content']\n",
    "labels = data['label']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(contents)\n",
    "\n",
    "contents_seq = tokenizer.texts_to_sequences(contents)\n",
    "\n",
    "# Padding sequences\n",
    "max_length = 100\n",
    "contents_padded = pad_sequences(contents_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "# Class weights for imbalanced dataset\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(labels),\n",
    "    y=labels\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(f\"Class Weights: {class_weights}\")\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=max_length),\n",
    "    Bidirectional(LSTM(128, return_sequences=False)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Early stopping\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model using the entire dataset as training data\n",
    "history = model.fit(\n",
    "    contents_padded, labels,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weights,\n",
    "    # callbacks=[early_stopping],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Evaluate the model using the same data\n",
    "test_loss, test_accuracy = model.evaluate(contents_padded, labels, verbose=2)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Predict on the entire dataset\n",
    "predictions = model.predict(contents_padded)\n",
    "predicted_labels = [\"Trolling\" if pred > 0.5 else \"Not Trolling\" for pred in predictions.flatten()]\n",
    "\n",
    "# Save the predictions as a DataFrame\n",
    "predictions_df = pd.DataFrame({\n",
    "    \"Content\": data['content'],\n",
    "    \"Actual Label\": [\"Trolling\" if lbl == 1 else \"Not Trolling\" for lbl in labels],\n",
    "    \"Predicted Label\": predicted_labels\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "predictions_df.to_csv(\"trolling_predictions.csv\", index=False)\n",
    "print(\"Predictions have been saved to 'trolling_predictions.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Export the model\n",
    "model.save(\"cyberv1_detection_model.h5\")\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the tokenizer\n",
    "with open(\"tokenizerv1.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(\"Tokenizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    12179\n",
      "1     7822\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Load the tokenizer\n",
    "with open(\"tokenizerv1.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "print(\"Tokenizer loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310ms/step\n",
      "Prediction: Not Cyberbullying\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the model\n",
    "loaded_model = load_model(\"cyberv1_detection_model.h5\")\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Test prediction using the loaded model\n",
    "sample_text = \"I think this is a bad idea.\"\n",
    "prediction = loaded_model.predict(pad_sequences(tokenizer.texts_to_sequences([sample_text]), maxlen=100, padding='post'))\n",
    "print(\"Prediction:\", \"Cyberbullying\" if prediction[0][0] > 0.5 else \"Not Cyberbullying\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "Prediction for sample text: Not Trolling\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction for sample text: Trolling\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediction for sample text: Not Trolling\n"
     ]
    }
   ],
   "source": [
    "# Test the prediction function\n",
    "sample_text = \"you are handsome\"\n",
    "print(f\"Prediction for sample text: {predict_trolling(sample_text)}\")\n",
    "\n",
    "sample_text = \"fuck you, i hate you so much\"\n",
    "print(f\"Prediction for sample text: {predict_trolling(sample_text)}\")\n",
    "\n",
    "\n",
    "sample_text = \"you are not handsome\"\n",
    "print(f\"Prediction for sample text: {predict_trolling(sample_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample tweets with and without cursing\n",
    "tweets = [\n",
    "    \"you are handsome\",\n",
    "    \"you are not handsome\",\n",
    "    \"You're such a loser, I can't believe anyone likes you.\",\n",
    "    \"Wow, that was an amazing game last night!\",\n",
    "    \"Why the hell would you even say that? You're so stupid.\",\n",
    "    \"This is the best day of my life. Thank you all for making it special!\",\n",
    "    \"Shut the fuck up already!\",\n",
    "    \"I just got promoted at work! Feeling so proud.\",\n",
    "    \"What an awful, pathetic excuse of a human you are.\",\n",
    "    \"The weather today is absolutely beautiful. Loving it!\",\n",
    "    \"You're a complete idiot, no one cares about you.\",\n",
    "    \"Celebrating my anniversary with my wonderful partner tonight. Love you!\",\n",
    "    \"What the fuck were you thinking, moron?\",\n",
    "    \"Feeling grateful for my family and friends.\",\n",
    "    \"I hate you so much, you absolute piece of garbage.\",\n",
    "    \"Just finished a marathon! So tired but feeling great.\",\n",
    "    \"This place is a fucking disaster. Who runs this dump?\",\n",
    "    \"Went hiking today and saw the most beautiful sunset.\",\n",
    "    \"You're so annoying, I wish you'd disappear forever.\",\n",
    "    \"Baking cookies for my friends. Can't wait to share them!\",\n",
    "    \"Can't believe how stupid you are, get a life.\",\n",
    "    \"Going to the movies tonight with friends. So excited!\",\n",
    "    \"You're so fucking dumb, it hurts my brain to listen to you.\",\n",
    "    \"Had a great meeting today at work. Feeling inspired.\",\n",
    "    \"Why don't you just shut your damn mouth already?\",\n",
    "    \"Feeling so blessed for this opportunity. Thank you, everyone!\",\n",
    "    \"You really fucked it up this time, didn't you?\",\n",
    "    \"The flowers in my garden are blooming beautifully this season.\",\n",
    "    \"You're a worthless piece of shit, just go away.\",\n",
    "    \"Enjoying a quiet evening reading my favorite book.\",\n",
    "    \"What the hell is wrong with you, seriously?\",\n",
    "    \"Had an amazing workout session today! Feeling great.\",\n",
    "    \"Fuck you and everything you stand for.\",\n",
    "    \"Loving this new recipe I tried. It's so delicious!\",\n",
    "    \"You're a joke, and nobody takes you seriously.\",\n",
    "    \"Spent the day volunteering at the animal shelter. So rewarding!\",\n",
    "    \"Why the fuck would anyone think you're competent?\",\n",
    "    \"Catching up on my favorite TV series tonight.\",\n",
    "    \"You're just a sad excuse for a person.\",\n",
    "    \"Had a fantastic time at the concert last night!\",\n",
    "    \"You're fucking pathetic, just quit already.\",\n",
    "    \"Grateful for all the good things happening in my life right now.\",\n",
    "    \"You're full of shit, and everyone knows it.\",\n",
    "    \"Taking my dog for a walk in the park. Such a peaceful evening.\",\n",
    "    \"What the fuck is your problem? Get a clue.\",\n",
    "    \"So happy to announce that I'll be starting my dream job next month!\",\n",
    "    \"You're a fucking embarrassment to everyone around you.\",\n",
    "    \"Planning a surprise birthday party for my best friend. Can't wait!\",\n",
    "    \"Nobody gives a shit about you or your stupid opinions.\",\n",
    "    \"Had the best pizza ever today. Life is good.\",\n",
    "    \"Fuck off and leave me alone, asshole.\",\n",
    "    \"Can't wait to see my family this weekend!\",\n",
    "    \"You're the worst kind of person, and I can't stand you.\",\n",
    "    \"Learning a new skill today. Feeling productive and happy.\",\n",
    "    \"You're a fucking waste of space.\",\n",
    "    \"Visited the museum today and saw some amazing artwork.\",\n",
    "    \"You're so full of yourself, it's fucking hilarious.\",\n",
    "    \"Spent the afternoon gardening. It was so relaxing.\",\n",
    "    \"You're a dumbass, and everyone knows it.\",\n",
    "    \"Excited to start a new project at work tomorrow.\",\n",
    "    \"Why don't you just fuck off already?\",\n",
    "    \"Had a wonderful picnic by the lake with friends.\",\n",
    "    \"You're fucking useless and always will be.\",\n",
    "    \"Just finished painting my room. It looks amazing!\",\n",
    "    \"You're a fucking liar, and nobody trusts you.\",\n",
    "    \"Feeling so accomplished after completing that big task today.\",\n",
    "    \"Why the fuck do you even try? You're terrible.\",\n",
    "    \"So proud of my little brother for graduating today!\",\n",
    "    \"You're a selfish piece of shit, and everyone hates you.\",\n",
    "    \"Made a new friend today at the park. Feeling great.\",\n",
    "    \"You're a fucking idiot, and it's embarrassing to know you.\",\n",
    "    \"Excited to attend the conference next week. Lots to learn!\",\n",
    "    \"You're a complete asshole, and everyone knows it.\",\n",
    "    \"Took some beautiful photos of nature during my hike today.\",\n",
    "    \"Why are you so fucking incompetent? It's infuriating.\",\n",
    "    \"Enjoyed a lovely dinner with my family tonight.\",\n",
    "    \"You're the biggest fucking moron I've ever met.\",\n",
    "    \"Started a new workout plan today. Feeling motivated!\",\n",
    "    \"You're so full of shit, it's ridiculous.\",\n",
    "    \"Attended a fantastic workshop on personal growth today.\",\n",
    "    \"Why don't you just shut the fuck up already?\",\n",
    "    \"Feeling happy and at peace with my life right now.\",\n",
    "    \"You're a piece of shit, and no one likes you.\",\n",
    "    \"Had a great time exploring the city with my best friend.\",\n",
    "    \"What the fuck is wrong with you? You're insufferable.\",\n",
    "    \"Baked some homemade bread today. It smells amazing.\",\n",
    "    \"You're such a fucking hypocrite, it's unreal.\",\n",
    "    \"Feeling relaxed after a long day at the spa.\",\n",
    "    \"You're a complete piece of shit, and nobody wants you around.\",\n",
    "    \"Spent the evening watching the stars. So beautiful and calming.\",\n",
    "    \"You're the most annoying fucking person I've ever dealt with.\",\n",
    "    \"Planning my dream vacation. Can't wait to travel again!\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "                                              content         prediction\n",
      "0                                    you are handsome  Not Cyberbullying\n",
      "1                                you are not handsome  Not Cyberbullying\n",
      "2   You're such a loser, I can't believe anyone li...  Not Cyberbullying\n",
      "3           Wow, that was an amazing game last night!  Not Cyberbullying\n",
      "4   Why the hell would you even say that? You're s...      Cyberbullying\n",
      "..                                                ...                ...\n",
      "87       Feeling relaxed after a long day at the spa.  Not Cyberbullying\n",
      "88  You're a complete piece of shit, and nobody wa...  Not Cyberbullying\n",
      "89  Spent the evening watching the stars. So beaut...  Not Cyberbullying\n",
      "90  You're the most annoying fucking person I've e...  Not Cyberbullying\n",
      "91  Planning my dream vacation. Can't wait to trav...  Not Cyberbullying\n",
      "\n",
      "[92 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Convert tweets to a DataFrame for better handling\n",
    "tweets_df = pd.DataFrame(tweets, columns=[\"content\"])\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "sequences = tokenizer.texts_to_sequences(tweets_df['content'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=100, padding='post')\n",
    "\n",
    "# Make predictions\n",
    "predictions = loaded_model.predict(padded_sequences)\n",
    "\n",
    "# Add predictions to the DataFrame\n",
    "tweets_df['prediction'] = [\"Cyberbullying\" if pred > 0.5 else \"Not Cyberbullying\" for pred in predictions]\n",
    "\n",
    "# Print results\n",
    "print(tweets_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets and predictions have been saved to 'tweet_predictions.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "tweets_df.to_csv(\"tweet_predictions.csv\", index=False)\n",
    "\n",
    "print(\"Tweets and predictions have been saved to 'tweet_predictions.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
