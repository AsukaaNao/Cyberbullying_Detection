{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_cyberbullying' 'gender' 'religion' 'other_cyberbullying' 'age'\n",
      " 'ethnicity']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/AsukaaNao/datasets/refs/heads/main/cyberbullying_tweets.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "categories=data['cyberbullying_type'].unique()\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution : [6321 6442 6432 6211 6389 6358]\n",
      "Class Weights: {0: 1.005985339872383, 1: 0.987089930663355, 2: 0.9886245854063018, 3: 1.0238018569205174, 4: 0.9952783429853393, 5: 1.0001310684701687}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bryan\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1193/1193 - 73s - 61ms/step - accuracy: 0.7508 - loss: 0.5957 - val_accuracy: 0.8218 - val_loss: 0.4452\n",
      "Epoch 2/20\n",
      "1193/1193 - 69s - 58ms/step - accuracy: 0.8553 - loss: 0.3728 - val_accuracy: 0.8373 - val_loss: 0.4226\n",
      "Epoch 3/20\n",
      "1193/1193 - 71s - 60ms/step - accuracy: 0.8823 - loss: 0.3055 - val_accuracy: 0.8337 - val_loss: 0.4428\n",
      "Epoch 4/20\n",
      "1193/1193 - 70s - 59ms/step - accuracy: 0.8974 - loss: 0.2637 - val_accuracy: 0.8270 - val_loss: 0.5145\n",
      "Epoch 5/20\n",
      "1193/1193 - 70s - 59ms/step - accuracy: 0.9079 - loss: 0.2317 - val_accuracy: 0.8265 - val_loss: 0.5570\n",
      "Epoch 6/20\n",
      "1193/1193 - 72s - 61ms/step - accuracy: 0.9149 - loss: 0.2144 - val_accuracy: 0.8237 - val_loss: 0.5784\n",
      "Epoch 7/20\n",
      "1193/1193 - 75s - 63ms/step - accuracy: 0.9210 - loss: 0.1917 - val_accuracy: 0.8216 - val_loss: 0.6197\n",
      "Epoch 8/20\n",
      "1193/1193 - 72s - 61ms/step - accuracy: 0.9261 - loss: 0.1737 - val_accuracy: 0.8180 - val_loss: 0.7220\n",
      "Epoch 9/20\n",
      "1193/1193 - 68s - 57ms/step - accuracy: 0.9301 - loss: 0.1645 - val_accuracy: 0.8186 - val_loss: 0.7966\n",
      "Epoch 10/20\n",
      "1193/1193 - 70s - 59ms/step - accuracy: 0.9323 - loss: 0.1538 - val_accuracy: 0.8158 - val_loss: 0.8239\n",
      "Epoch 11/20\n",
      "1193/1193 - 69s - 58ms/step - accuracy: 0.9367 - loss: 0.1399 - val_accuracy: 0.8148 - val_loss: 0.9474\n",
      "Epoch 12/20\n",
      "1193/1193 - 70s - 58ms/step - accuracy: 0.9390 - loss: 0.1313 - val_accuracy: 0.8110 - val_loss: 1.0247\n",
      "Epoch 13/20\n",
      "1193/1193 - 70s - 59ms/step - accuracy: 0.9409 - loss: 0.1265 - val_accuracy: 0.8157 - val_loss: 1.0353\n",
      "Epoch 14/20\n",
      "1193/1193 - 71s - 59ms/step - accuracy: 0.9428 - loss: 0.1194 - val_accuracy: 0.8122 - val_loss: 1.2228\n",
      "Epoch 15/20\n",
      "1193/1193 - 71s - 59ms/step - accuracy: 0.9441 - loss: 0.1179 - val_accuracy: 0.8107 - val_loss: 1.1430\n",
      "Epoch 16/20\n",
      "1193/1193 - 71s - 60ms/step - accuracy: 0.9452 - loss: 0.1083 - val_accuracy: 0.8005 - val_loss: 1.2593\n",
      "Epoch 17/20\n",
      "1193/1193 - 72s - 61ms/step - accuracy: 0.9474 - loss: 0.1074 - val_accuracy: 0.8152 - val_loss: 1.2411\n",
      "Epoch 18/20\n",
      "1193/1193 - 71s - 59ms/step - accuracy: 0.9493 - loss: 0.1023 - val_accuracy: 0.8120 - val_loss: 1.2602\n",
      "Epoch 19/20\n",
      "1193/1193 - 69s - 58ms/step - accuracy: 0.9494 - loss: 0.0998 - val_accuracy: 0.8117 - val_loss: 1.3666\n",
      "Epoch 20/20\n",
      "1193/1193 - 70s - 59ms/step - accuracy: 0.9509 - loss: 0.0954 - val_accuracy: 0.8097 - val_loss: 1.3420\n",
      "299/299 - 5s - 18ms/step - accuracy: 0.8097 - loss: 1.3420\n",
      "Test Loss: 1.342010259628296\n",
      "Test Accuracy: 0.8097285032272339\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step\n",
      "Prediction for sample text: gender\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/AsukaaNao/datasets/refs/heads/main/cyberbullying_tweets.csv\"\n",
    "data = pd.read_csv(url)\n",
    "data['tweet_text'] = data['tweet_text'].astype(str)\n",
    "\n",
    "# Text preprocessing\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)  # Remove non-alphabetical characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.split()  # Tokenize\n",
    "    text = [word for word in text if word not in stop_words]  # Remove stopwords\n",
    "    return \" \".join(text)\n",
    "\n",
    "data['tweet_text'] = data['tweet_text'].apply(preprocess_text)\n",
    "\n",
    "# Extract content and label\n",
    "contents = data['tweet_text']\n",
    "labels = data['cyberbullying_type']\n",
    "\n",
    "# Define categories\n",
    "categories = ['not_cyberbullying', 'gender', 'religion', 'other_cyberbullying', 'age', 'ethnicity']\n",
    "\n",
    "# Encode labels\n",
    "labels = pd.Categorical(labels, categories=categories).codes\n",
    "\n",
    "# Convert to one-hot encoding\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(contents, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Padding sequences\n",
    "max_length = 100\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "# Check class distribution\n",
    "print(f\"Class distribution : {np.bincount(y_train.argmax(axis=1))}\")\n",
    "\n",
    "# # Oversample minority class\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_train_resampled, y_train_resampled = smote.fit_resample(X_train_padded, y_train)\n",
    "\n",
    "# # Check new class distribution\n",
    "# print(f\"Class distribution after SMOTE: {np.bincount(y_train_resampled.argmax(axis=1))}\")\n",
    "\n",
    "# Class weights for imbalanced dataset\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.arange(len(categories)),\n",
    "    y=y_train.argmax(axis=1)\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(f\"Class Weights: {class_weights}\")\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=max_length),\n",
    "    Bidirectional(LSTM(128, return_sequences=False)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(categories), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Early stopping (uncomment if you want to stop early based on validation loss)\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_padded, y_train,\n",
    "    # X_train_resampled, y_train_resampled,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_padded, y_test),\n",
    "    class_weight=class_weights,\n",
    "    # callbacks=[early_stopping],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test_padded, y_test, verbose=2)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Prediction function\n",
    "def predict_cyberbullying(content):\n",
    "    content = preprocess_text(content)\n",
    "    seq = tokenizer.texts_to_sequences([content])\n",
    "    padded = pad_sequences(seq, maxlen=max_length, padding='post')\n",
    "    prediction = model.predict(padded)\n",
    "    return categories[np.argmax(prediction)]\n",
    "\n",
    "# Test the prediction function\n",
    "sample_text = \"You are so dumb and useless!\"\n",
    "print(f\"Prediction for sample text: {predict_cyberbullying(sample_text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predict_cyberbullying(content):\n",
    "    content = preprocess_text(content)\n",
    "    seq = tokenizer.texts_to_sequences([content])\n",
    "    padded = pad_sequences(seq, maxlen=max_length, padding='post')\n",
    "    prediction = model.predict(padded)\n",
    "    predicted_label_index = np.argmax(prediction)\n",
    "    return categories[predicted_label_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Prediction for sample text: other_cyberbullying\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Prediction for sample text: religion\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Prediction for sample text: other_cyberbullying\n"
     ]
    }
   ],
   "source": [
    "# Test the prediction function\n",
    "sample_text = \"idiot sandwich\"\n",
    "print(f\"Prediction for sample text: {predict_cyberbullying(sample_text)}\")\n",
    "\n",
    "sample_text = \"moslem terrorist stfu\"\n",
    "print(f\"Prediction for sample text: {predict_cyberbullying(sample_text)}\")\n",
    "\n",
    "sample_text = \"bruh fucking retartd\"\n",
    "print(f\"Prediction for sample text: {predict_cyberbullying(sample_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data as x_test (blm ganti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import re\n",
    "# import nltk\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# # Load the dataset\n",
    "# url = \"https://raw.githubusercontent.com/AsukaaNao/datasets/refs/heads/main/trolling_data.csv\"\n",
    "# data = pd.read_csv(url)\n",
    "# data['content'] = data['content'].astype(str)\n",
    "\n",
    "# # Text preprocessing\n",
    "# nltk.download('stopwords')\n",
    "# from nltk.corpus import stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "#     text = re.sub(r\"[^a-zA-Z]\", \" \", text)  # Remove non-alphabetical characters\n",
    "#     text = text.lower()  # Convert to lowercase\n",
    "#     text = text.split()  # Tokenize\n",
    "#     text = [word for word in text if word not in stop_words]  # Remove stopwords\n",
    "#     return \" \".join(text)\n",
    "\n",
    "# data['content'] = data['content'].apply(preprocess_text)\n",
    "\n",
    "# # Extract content and label\n",
    "# contents = data['content']\n",
    "# labels = data['label']\n",
    "\n",
    "# # Encode labels\n",
    "# label_encoder = LabelEncoder()\n",
    "# labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# # Tokenization\n",
    "# tokenizer = Tokenizer(num_words=10000)\n",
    "# tokenizer.fit_on_texts(contents)\n",
    "\n",
    "# contents_seq = tokenizer.texts_to_sequences(contents)\n",
    "\n",
    "# # Padding sequences\n",
    "# max_length = 100\n",
    "# contents_padded = pad_sequences(contents_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "# # Class weights for imbalanced dataset\n",
    "# class_weights = compute_class_weight(\n",
    "#     class_weight='balanced',\n",
    "#     classes=np.unique(labels),\n",
    "#     y=labels\n",
    "# )\n",
    "# class_weights = dict(enumerate(class_weights))\n",
    "# print(f\"Class Weights: {class_weights}\")\n",
    "\n",
    "# # Build the model\n",
    "# model = Sequential([\n",
    "#     Embedding(input_dim=10000, output_dim=128, input_length=max_length),\n",
    "#     Bidirectional(LSTM(128, return_sequences=False)),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Print model summary\n",
    "# model.summary()\n",
    "\n",
    "# # Early stopping\n",
    "# # early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# # Train the model using the entire dataset as training data\n",
    "# history = model.fit(\n",
    "#     contents_padded, labels,\n",
    "#     epochs=20,\n",
    "#     batch_size=32,\n",
    "#     class_weight=class_weights,\n",
    "#     # callbacks=[early_stopping],\n",
    "#     verbose=2\n",
    "# )\n",
    "\n",
    "# # Evaluate the model using the same data\n",
    "# test_loss, test_accuracy = model.evaluate(contents_padded, labels, verbose=2)\n",
    "# print(f\"Test Loss: {test_loss}\")\n",
    "# print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# # Predict on the entire dataset\n",
    "# predictions = model.predict(contents_padded)\n",
    "# predicted_labels = [\"Trolling\" if pred > 0.5 else \"Not Trolling\" for pred in predictions.flatten()]\n",
    "\n",
    "# # Save the predictions as a DataFrame\n",
    "# predictions_df = pd.DataFrame({\n",
    "#     \"Content\": data['content'],\n",
    "#     \"Actual Label\": [\"Trolling\" if lbl == 1 else \"Not Trolling\" for lbl in labels],\n",
    "#     \"Predicted Label\": predicted_labels\n",
    "# })\n",
    "\n",
    "# # Save the DataFrame to a CSV file\n",
    "# predictions_df.to_csv(\"trolling_predictions.csv\", index=False)\n",
    "# print(\"Predictions have been saved to 'trolling_predictions.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export the model\n",
    "# model.save(\"cyberv1_detection_model.h5\")\n",
    "# print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Save the tokenizer\n",
    "# with open(\"tokenizerv1.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(tokenizer, f)\n",
    "# print(\"Tokenizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# # Load the tokenizer\n",
    "# with open(\"tokenizerv1.pkl\", \"rb\") as f:\n",
    "#     tokenizer = pickle.load(f)\n",
    "# print(\"Tokenizer loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# # Load the model\n",
    "# loaded_model = load_model(\"cyberv1_detection_model.h5\")\n",
    "# print(\"Model loaded successfully!\")\n",
    "\n",
    "# # Test prediction using the loaded model\n",
    "# sample_text = \"I think this is a bad idea.\"\n",
    "# prediction = loaded_model.predict(pad_sequences(tokenizer.texts_to_sequences([sample_text]), maxlen=100, padding='post'))\n",
    "# print(\"Prediction:\", \"Cyberbullying\" if prediction[0][0] > 0.5 else \"Not Cyberbullying\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the prediction function\n",
    "# sample_text = \"you are handsome\"\n",
    "# print(f\"Prediction for sample text: {predict_trolling(sample_text)}\")\n",
    "\n",
    "# sample_text = \"fuck you, i hate you so much\"\n",
    "# print(f\"Prediction for sample text: {predict_trolling(sample_text)}\")\n",
    "\n",
    "\n",
    "# sample_text = \"you are not handsome\"\n",
    "# print(f\"Prediction for sample text: {predict_trolling(sample_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample tweets with and without cursing\n",
    "# tweets = [\n",
    "#     \"you are handsome\",\n",
    "#     \"you are not handsome\",\n",
    "#     \"You're such a loser, I can't believe anyone likes you.\",\n",
    "#     \"Wow, that was an amazing game last night!\",\n",
    "#     \"Why the hell would you even say that? You're so stupid.\",\n",
    "#     \"This is the best day of my life. Thank you all for making it special!\",\n",
    "#     \"Shut the fuck up already!\",\n",
    "#     \"I just got promoted at work! Feeling so proud.\",\n",
    "#     \"What an awful, pathetic excuse of a human you are.\",\n",
    "#     \"The weather today is absolutely beautiful. Loving it!\",\n",
    "#     \"You're a complete idiot, no one cares about you.\",\n",
    "#     \"Celebrating my anniversary with my wonderful partner tonight. Love you!\",\n",
    "#     \"What the fuck were you thinking, moron?\",\n",
    "#     \"Feeling grateful for my family and friends.\",\n",
    "#     \"I hate you so much, you absolute piece of garbage.\",\n",
    "#     \"Just finished a marathon! So tired but feeling great.\",\n",
    "#     \"This place is a fucking disaster. Who runs this dump?\",\n",
    "#     \"Went hiking today and saw the most beautiful sunset.\",\n",
    "#     \"You're so annoying, I wish you'd disappear forever.\",\n",
    "#     \"Baking cookies for my friends. Can't wait to share them!\",\n",
    "#     \"Can't believe how stupid you are, get a life.\",\n",
    "#     \"Going to the movies tonight with friends. So excited!\",\n",
    "#     \"You're so fucking dumb, it hurts my brain to listen to you.\",\n",
    "#     \"Had a great meeting today at work. Feeling inspired.\",\n",
    "#     \"Why don't you just shut your damn mouth already?\",\n",
    "#     \"Feeling so blessed for this opportunity. Thank you, everyone!\",\n",
    "#     \"You really fucked it up this time, didn't you?\",\n",
    "#     \"The flowers in my garden are blooming beautifully this season.\",\n",
    "#     \"You're a worthless piece of shit, just go away.\",\n",
    "#     \"Enjoying a quiet evening reading my favorite book.\",\n",
    "#     \"What the hell is wrong with you, seriously?\",\n",
    "#     \"Had an amazing workout session today! Feeling great.\",\n",
    "#     \"Fuck you and everything you stand for.\",\n",
    "#     \"Loving this new recipe I tried. It's so delicious!\",\n",
    "#     \"You're a joke, and nobody takes you seriously.\",\n",
    "#     \"Spent the day volunteering at the animal shelter. So rewarding!\",\n",
    "#     \"Why the fuck would anyone think you're competent?\",\n",
    "#     \"Catching up on my favorite TV series tonight.\",\n",
    "#     \"You're just a sad excuse for a person.\",\n",
    "#     \"Had a fantastic time at the concert last night!\",\n",
    "#     \"You're fucking pathetic, just quit already.\",\n",
    "#     \"Grateful for all the good things happening in my life right now.\",\n",
    "#     \"You're full of shit, and everyone knows it.\",\n",
    "#     \"Taking my dog for a walk in the park. Such a peaceful evening.\",\n",
    "#     \"What the fuck is your problem? Get a clue.\",\n",
    "#     \"So happy to announce that I'll be starting my dream job next month!\",\n",
    "#     \"You're a fucking embarrassment to everyone around you.\",\n",
    "#     \"Planning a surprise birthday party for my best friend. Can't wait!\",\n",
    "#     \"Nobody gives a shit about you or your stupid opinions.\",\n",
    "#     \"Had the best pizza ever today. Life is good.\",\n",
    "#     \"Fuck off and leave me alone, asshole.\",\n",
    "#     \"Can't wait to see my family this weekend!\",\n",
    "#     \"You're the worst kind of person, and I can't stand you.\",\n",
    "#     \"Learning a new skill today. Feeling productive and happy.\",\n",
    "#     \"You're a fucking waste of space.\",\n",
    "#     \"Visited the museum today and saw some amazing artwork.\",\n",
    "#     \"You're so full of yourself, it's fucking hilarious.\",\n",
    "#     \"Spent the afternoon gardening. It was so relaxing.\",\n",
    "#     \"You're a dumbass, and everyone knows it.\",\n",
    "#     \"Excited to start a new project at work tomorrow.\",\n",
    "#     \"Why don't you just fuck off already?\",\n",
    "#     \"Had a wonderful picnic by the lake with friends.\",\n",
    "#     \"You're fucking useless and always will be.\",\n",
    "#     \"Just finished painting my room. It looks amazing!\",\n",
    "#     \"You're a fucking liar, and nobody trusts you.\",\n",
    "#     \"Feeling so accomplished after completing that big task today.\",\n",
    "#     \"Why the fuck do you even try? You're terrible.\",\n",
    "#     \"So proud of my little brother for graduating today!\",\n",
    "#     \"You're a selfish piece of shit, and everyone hates you.\",\n",
    "#     \"Made a new friend today at the park. Feeling great.\",\n",
    "#     \"You're a fucking idiot, and it's embarrassing to know you.\",\n",
    "#     \"Excited to attend the conference next week. Lots to learn!\",\n",
    "#     \"You're a complete asshole, and everyone knows it.\",\n",
    "#     \"Took some beautiful photos of nature during my hike today.\",\n",
    "#     \"Why are you so fucking incompetent? It's infuriating.\",\n",
    "#     \"Enjoyed a lovely dinner with my family tonight.\",\n",
    "#     \"You're the biggest fucking moron I've ever met.\",\n",
    "#     \"Started a new workout plan today. Feeling motivated!\",\n",
    "#     \"You're so full of shit, it's ridiculous.\",\n",
    "#     \"Attended a fantastic workshop on personal growth today.\",\n",
    "#     \"Why don't you just shut the fuck up already?\",\n",
    "#     \"Feeling happy and at peace with my life right now.\",\n",
    "#     \"You're a piece of shit, and no one likes you.\",\n",
    "#     \"Had a great time exploring the city with my best friend.\",\n",
    "#     \"What the fuck is wrong with you? You're insufferable.\",\n",
    "#     \"Baked some homemade bread today. It smells amazing.\",\n",
    "#     \"You're such a fucking hypocrite, it's unreal.\",\n",
    "#     \"Feeling relaxed after a long day at the spa.\",\n",
    "#     \"You're a complete piece of shit, and nobody wants you around.\",\n",
    "#     \"Spent the evening watching the stars. So beautiful and calming.\",\n",
    "#     \"You're the most annoying fucking person I've ever dealt with.\",\n",
    "#     \"Planning my dream vacation. Can't wait to travel again!\"\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# # Convert tweets to a DataFrame for better handling\n",
    "# tweets_df = pd.DataFrame(tweets, columns=[\"content\"])\n",
    "\n",
    "# # Tokenize and pad sequences\n",
    "# sequences = tokenizer.texts_to_sequences(tweets_df['content'])\n",
    "# padded_sequences = pad_sequences(sequences, maxlen=100, padding='post')\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = loaded_model.predict(padded_sequences)\n",
    "\n",
    "# # Add predictions to the DataFrame\n",
    "# tweets_df['prediction'] = [\"Cyberbullying\" if pred > 0.5 else \"Not Cyberbullying\" for pred in predictions]\n",
    "\n",
    "# # Print results\n",
    "# print(tweets_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the DataFrame to a CSV file\n",
    "# tweets_df.to_csv(\"tweet_predictions.csv\", index=False)\n",
    "\n",
    "# print(\"Tweets and predictions have been saved to 'tweet_predictions.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
